\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[none]{hyphenat}
\usepackage{amsthm}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{cleveref}
\usepackage{listings, xcolor}
\usepackage{graphicx}
\graphicspath{ {images/} }
\hypersetup{citecolor=blue}
\hypersetup{colorlinks=true}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

\theoremstyle{remark}
\newtheorem{remark}{Remark}

\theoremstyle{remark}
\newtheorem{question}{Question}

\newtheorem{theorem}{Theorem}[section]

\theoremstyle{example}
\newtheorem{example}{Example}

\title{Exercises}
\author{AndrÃ©s Tamayo }

\begin{document}

\maketitle

\section*{Exercise 1}

\subsection*{Excercise e)}

\begin{question}
\textit{Show that Mahalanobis distance is a metric.}
\end{question}

Mahalanobis distance can be defined as a dissimilarity measure between two random vectors $\vec{x}$ and $\vec{y}$ of the same distribution with the covariance matrix $S$:
\[
d(\vec{x},\vec{y}) = \sqrt{(\vec{x} - \vec{y})^{T}S^{-1}(\vec{x}-\vec{y})}
\]

\begin{proof}
In order to prove that $d(\vec{x},\vec{y}) \geq 0$ we have to show that $(\vec{x} - \vec{y})^{T}S^{-1}(\vec{x}-\vec{y})\geq 0$. Note that this holds if we prove that $S^{-1}$ is definite positive. Let's consider a sample of vector $x_{i} = (x_{i1},\ldots,x_{ik})^{T}$, with $i = 1,\ldots,n$, the sample mean vector is
\[
\overline{x} = \dfrac{1}{n} \sum_{i = 1}^{n} x_{i},
\]
and the sample covariance matrix is
\[
S = \dfrac{1}{n} \sum_{i = 1}^{n} (x_{i} - \overline{x})(x_{i}- \overline{x})^{T}
\]
Then, for a nonzero vector $y \in \mathbb{R}^{n}$, we have
\begin{align*}
    y^{T}S y &= y^{T} \left ( \dfrac{1}{n} \sum_{i = 1}^{n} (x_{i} - \overline{x})(x_{i}- \overline{x})^{T} \right) y \\
    &= \dfrac{1}{n} \sum_{i = 1}^{n} y^{T} (x_{i} - \overline{x})(x_{i} - \overline{x})^{T} y \\
    &= \dfrac{1}{n} \sum_{i = 1}^{n} ((x_{i} - \overline{x})^{T}y)^{2} \geq 0 \quad \textbf{*}
\end{align*}
By this, $S$ is always positive semi-definite. Now, we have to show that $S$ is definite. Let's define $z_{i} = (x_{i} - \overline{x})$, for $i = 1,\ldots,n$. For any nonzero $y \in \mathbb{R}^{n}$, \textbf{(*)} is zero iff $z_{i}^{T}y = 0$, for each $i = 1,\ldots,n$. Let's suppose now that the set $\{z_{1},\ldots,z_{n}\}$ spans over $\mathbb{R}^{n}$. Then there are real numbers $\alpha_{1},\ldots,\alpha_{n}$ such hat $y = \alpha_{1}z_{1} + \ldots + \alpha_{n}z_{n}$. But then we have $y^{T}y = \alpha_{1}z_{1}^{T}y + \ldots + \alpha_{n}z_{n}^{T}y = 0$, which yields that $y = 0$, a contradiction. Hence, if the $z_{i}$ spans over $\mathbb{R}^{n}$, then $S$ is positive definite. \\
We conclude that $S$ is a definite positive matrix, hence
\[
d(\vec{x},\overline{x}) \geq 0
\]
We have to prove now that $d(\vec{x},\vec{y}) = 0$ iff  $\vec{x} = \vec{y}$. This is obvious from the definition of the mahalanobis distance, because
\[
d(\vec{x},\vec{y}) = \sqrt{(\vec{x} - \vec{y})^{T}S^{-1}(\vec{x}-\vec{y})} = \sqrt{(\vec{x} - \vec{x})^{T}S^{-1}(\vec{x}-\vec{x})}
\]
since $\vec{x}$ and $\vec{y}$ have the same dimensions. \\

We continue with the proof that $d(\vec{x},\vec{y}) = d(\vec{y},\vec{x})$. This holds because of $S$ is a symmetric matrix, and hence we have finish. \\

At last, we have to prove the triangle inequality. Let $S$ be a symmetric $n \times n$ matrix (This because the definition of covariance matrix). And let's rename the mahalanobis norm as
\[
||x||_{S} = \sqrt{x^{T}Sx}
\]
We have shown that $S$ is positive-definite. By spectral theorem for symmetrix matrices, there are a diagonal $n \times n$ matrix $\Lambda = diag(\lambda_{1},\ldots,\lambda_{n})$ and an orthogonal $n \times n$ matrix $Q$ (i.e. $Q^{T}Q = I$), such that $Q^{T} = Q^{-1}$ and:
\[
S = Q^{T} \Lambda Q
\]
Because of the matrix $S$ is positive-definite we have that
\begin{align*}
    \lambda_{1} &> 0 \\
    \lambda_{2} &> 0 \\
    \ldots \\
    \lambda_{n} &> 0
\end{align*}
Let the matrix
\[
U = diag(\sqrt{\lambda_{1}},\sqrt{\lambda_{2}},\ldots,\sqrt{\lambda_{n}})Q,
\]
note that:
\[
S = U^{T}U
\]
set now $\overline{x} = Ux$ and $\overline{y} = Uy$. Let $||v||_{E} = \sqrt{v_{1}^{2} + v_{2}^{2} + \ldots + v_{n}^{n}}$ the usual euclidean distance. Then is clearly that
\begin{align*}
    ||x||_{S} &= ||\overline{x}||_{E} \\
    ||y||_{S} &= ||\overline{y}||_{E} \\
    ||x+y||_{S} &= ||\overline{x} + \overline{y}||_{E} \quad \textbf{(**)}
\end{align*}
By usual triangular inequality we have:
\[
||\overline{x} + \overline{y}||_{E} \leq ||\overline{x}||_{E} + ||\overline{y}||_{E}
\]
By the equality \textbf{(**)} we have
\[
||x + y||_{S} \leq ||x||_{S} + ||y||_{S}
\]
\end{proof}


\subsection*{Exercise g)} 

\begin{question}
\textit{If $d : X \times X \to \mathbb{R}$ is a metric, then $\overline{d}(x,y) = \min{\{1,d(x,y)\}}$ also is.}
\end{question}
\begin{proof}
Let's show that $\overline{d}(x,y) = \min{\{1,d(x,y)\}}$ is a metric
\begin{itemize}
    \item $\overline{d}(x,y) \geq 0$. We have three cases.
    \begin{itemize}
        \item If $1 = d(x,y)$ then $\min{\{1,d(x,y)\}} = 1$, therefore $\overline{d}(x,y) \geq 0$.
        \item If $1 < d(x,y)$ then $\min{\{1,d(x,y)\}} = 1$, therefore $\overline{d}(x,y) \geq 0$.
        \item If $d(x,y) < 1$ then $\min{\{1,d(x,y)\}} = d(x,y)$. We have that $d(x,y)$ is a metric, $d(x,y) \geq 0$, therefore $\overline{d}(x,y) \geq 0$
    \end{itemize}
    \item $\overline{d}(x,y) = 0$ iff $x = y$.
    \begin{itemize}
        \item $\overline{d}(x,y) = 0 \Rightarrow x = y$. \\
        
        We have that $\overline{d}(x,y) = 0$, but this means $\min{\{1,d(x,y)\}} = 0$. Clearly $1 \neq 0$. Then $d(x,y) = 0$ iff $x = y$, but $d$ is a metric. Therefore $x = y$.
        \item $x = y \Rightarrow \overline{d}(x,y) = 0$. \\
        
        Let's suppose that $x = y$, then $\overline{d}(x,y) = \min{\{1,d(x,y)\}} = 0$. This because $d$ is a metric, and therefore $d(x,y) = 0$ if $x = y$ hence $\overline{d}(x,y) = 0$.
    \end{itemize}
    \item $\overline{d}(x,y) = \overline{d}(y,x)$. \\
    
    $\overline{d}(x,y) = \min{\{1,d(x,y)\}}$, because $d$ is a metric $d(x,y) = d(y,x)$, then $\overline{d}(x,y) = \min{\{1,d(x,y)\}} = \min{\{1,d(y,x)\}} = \overline{d}(y,x)$. \\
    
    \item $\overline{d}(x,z) \leq \overline{d}(x,y) + \overline{d}(y,z)$. \\
    
    $\overline{d}(x,z) = \min{\{1,d(x,z)\}}$. Because $d$ is a metric we have $d(x,z) \leq d(x,y) + d(y,z)$. Therefore
    \begin{align*}
        \overline{d}(x,z) &\leq \min{\{1,d(x,y) + d(y,z)\}} \\
        &\leq \min{\{1,d(x,y)\}} + \min{\{1,d(y,z)\}} \\
        &= \overline{d}(x,y) + \overline{d}(y,z)
    \end{align*}
\end{itemize} 
\end{proof}

\subsection*{Exercise h)} 
\begin{question}
If $A \subset B$, both subsets of $\mathbb{R}^{n}$, then for any $x \in \mathbb{R}^{n}$ and $d$ a metric, we have that $d(x,A) \geq d(x,B)$.
\end{question}

\begin{proof}
By the definition of distance between sets we know that 
\[d(x,B) = inf \{ d(x,b) : x \in \mathbb{R}^{n}, \quad b \in B \} \]
Since $d$ is a metric, we have that
\begin{align*}
    d(x,B) &\leq inf \{d(x,a) + d(a,b) : x \in \mathbb{R}^{n}, \quad a \in A, \quad b \in B \}
\end{align*}
By properties of the infimum
\begin{align*}
    d(x,B) &\leq inf \{ d(x,a) : a \in A, \quad x \in \mathbb{R}^{n}\} + inf \{ d(a,b) : a \in A, \quad b \in B\} \\
    &\leq d(x,A) + d(x,B)
\end{align*}
But since $A \subset B$ we have
\[
d(x,B) \leq d(x,A)
\]
\end{proof}

\section*{Exercise 2}

\begin{question}
Define what it is a pseudometric and show a few examples of pseudometrics.
\end{question}

\begin{definition}
A pseudometrix space $(E,d)$ is a set $E$ together with a non-negative real-valued function $d : E \times E \to \mathbb{R}_{\geq 0}$ (called a \textbf{pseudometric}) such that for every $x,y,z \in E$,
\begin{enumerate}
    \item $d(x,y) \geq 0$
    \item $d(x,x) = 0$
    \item $d(x,y) = d(y,x)$
    \item $d(x,z) \leq d(x,y) + d(y,z)$
\end{enumerate}
\end{definition}

\begin{example}
For a set $E$, define $d(x,y) = 0$ for all $x,y \in E$. We call $d$ the trivial pseudometric on $E$: all distancs are $0$.
\end{example}

\begin{example}
Every measure space $\Omega,\mathcal{A},\mu)$ can be viewed as a pseudometric space by defining
\[
d(A,B) := \mu(A \bigtriangleup B)
\]
for all $A,B \in \mathcal{A}$, where the triangle denotes symmetric difference.
\end{example}

\begin{example}
For vector spaces $V$, a seminorm $p$ induces a pseudometric on $V$, as
\[
d(x,y) = p(x - y)
\]
\end{example}
\end{document}
